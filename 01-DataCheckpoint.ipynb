{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "Team list:\n",
    "- Nikita Jos\n",
    "- Nancy Le\n",
    "- Mihikia Devireddy\n",
    "- Amey Gupta\n",
    "- Aryav Srinivas\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do FAANG companies differ in their daily stock returns and volatility during favorable versus unfavorable market conditions?\n",
    "\n",
    " Using adjusted closing prices, we will calculate daily percentage returns and compare the average returns and standard deviations for each company across different market periods. Market conditions will be defined using benchmark market trends (such as sustained upward or downward movement in the broader stock market). Statistical tests and regression analysis will be used to determine whether certain companies demonstrate greater stability or resilience during downturns. This is an inference-focused analysis that seeks to identify significant differences in performance rather than predict future stock prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "“FAANG” is an slang term which refers to the five most prominent (and best performing) American companies in the tech industry: ‘Facebook’, ‘Apple', ‘Amazon’, ‘Netflix’ and ‘Google’. These companies are among the largest worldwide, boasting a market capitalization of around $12 trillion.[1] Together the companies are worth a fifth of the total value of the S&P 500. The S&P 500 is a stock market index tracking the stocks of 500 leading companies listed on stock exchanges in the United States. \n",
    "\t\n",
    "Over the past decade, FAANG stocks have delivered impressive results, often outperforming the broader market. During bull markets they have been the primary drivers of index growth and when the market has downturns, their immense cash reserves and strong balance sheets help them recover faster than many other competitors. [2] This large influence means that changes in FAANG stock prices can heavily affect the S&P 500’s performance.[1]\n",
    "\t\n",
    "Despite their strong performance, FANG stocks historically have heightened volatility. Volatility refers to the extent to which a stock price fluctuates over time and is commonly used as a measurement of risk. Investors have expressed concerns about overvaluing the FAANG stocks can lead to systemic instability when facing market-wide events. \n",
    "Prior research has extensively examined FAANG’s influence on market indices, overall volatility behavior across market regimes, and the heightened risk characteristics of technology and growth stocks. However, relatively few studies directly compare how the volatility of individual FAANG companies differs across favorable, unfavorable, and stable market conditions, despite substantial differences in their business models and revenue drivers. [2] Analysts have taken note that speedy growth and high valuations in large tech companies can add to instability, in particular during periods of adverse economic conditions where investor behavior shifts in moments. Academic work on the subject also supports the notion that stock behavior alters dependant on the market and whether it is we are seeing bull, bear, or stable conditions. However, much fewer studies offer a direct comparison for how each individual FAANG company’s volatility may be different across various market regimes, despite their distinct business models.\n",
    "While FAANG companies are often grouped together, less is known about whether their stock volatility responds similarly to different market conditions or whether company-specific characteristics lead to distinct risk profiles. In this project, we will quantify volatility using daily log returns and compare how each FAANG company’s volatility changes across market regimes. The daily log return will be used to identify how much the stock moved each day which will then be used to quantify volatility through the standard deviation. Market conditions will be classified using the S&P 500’s return distribution (or VIX levels), allowing us to test whether volatility responses differ significantly between companies. VIX measures the market's expectation of 30-day volatility for the S&P 500. A VIX value greater than 30 indicates market fear and uncertainty, while values below 20 suggest stability. \n",
    "\n",
    "[1]https://www.ig.com/en/trading-strategies/what-are-the-faang-stocks-and-how-can-you-invest-in-them--250501#:~:text=stocks%20summed%20up-,What%20Are%20the%20FAANG%20Stocks?,diversified%20as%20a%20package%20deal\n",
    "\n",
    "[2] https://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.1989.tb02647.x "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hypothesize that FAANG companies with more diversified revenue streams, such as Amazon and Apple, will exhibit lower volatility and smaller negative daily returns during unfavorable market conditions compared to companies that rely more heavily on advertising or subscription-based revenue. Because diversified firms are generally less sensitive to shocks in any single sector, they are expected to show greater stability when the overall market declines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with descriptions of your actual datasets.\n",
    "\n",
    "For each dataset include the following information\n",
    "- Dataset #1\n",
    "  - Dataset Name: Apple.csv\n",
    "  - Link to the dataset:https://www.kaggle.com/datasets/aayushmishra1512/faang-complete-stock-data?resource=download&select=Apple.csv\n",
    "  - Number of observations: 10,016\n",
    "  - Number of variables:7\n",
    "  - Most Relevant Variables: Date, High, Low, Open, Close, Adj Close, Volume\n",
    "  - Descriptions of any shortcomings this dataset has with repsect to the project\n",
    "- Dataset #2\n",
    "  - Dataset Name: Amazon.csv\n",
    "  - Link to the dataset: https://www.kaggle.com/datasets/aayushmishra1512/faang-complete-stock-data?select=Amazon.csv\n",
    "  - Number of observations: 5852\n",
    "  - Number of variables: 7\n",
    "  - Most Relevant Variables: Date, High, Low, Open, Close, Adj Close, Volume\n",
    "  - Descriptions of any shortcomings this dataset has with respect to the project: While the dataset includes daily stock prices over multiple years, it may not fully cover all types of market conditions (e.g., extreme recessions, rapid recoveries) needed to robustly classify “favorable” vs. “unfavorable” periods. COVID-19 pandemic years (2020–2022) are partially represented.\n",
    "- Dataset #3 (if you have more than one!)\n",
    "  - Dataset Name: Facebook.csv\n",
    "  - Link to the dataset: https://www.kaggle.com/datasets/aayushmishra1512/faang-complete-stock-data?resource=download&select=Facebook.csv\n",
    "  - Number of observations: 2076\n",
    "  - Number of variables: 7\n",
    "  - Most Relevant Variables: Date, High, Low, Open, Close, Adj Close, Volume\n",
    "  - Potential dataset limitations:\n",
    "    - Only includes trading days (no weekends/holidays)\n",
    "    - Does not include macroeconomic variables (inflation, interest rates)\n",
    "    - Early IPO period shows unusually high volatility compared to later stable periods\n",
    "- Dataset #4\n",
    "  - Dataset Name: Netflix.csv\n",
    "  - Link to the dataset: https://www.kaggle.com/datasets/aayushmishra1512/faang-complete-stock-data?resource=download&select=Netflix.csv\n",
    "  - Number of observations: 4,581\n",
    "  - Number of variables: 7\n",
    "  - Most Relevant Variables: Date, Open, High, Low, Close, Adj Close, Volume\n",
    "  - Potential dataset limitations:\n",
    "    - Only includes trading days (no weekends or market holidays)\n",
    "    - Does not include macroeconomic variables (e.g., interest rates, inflation, VIX) needed to classify market conditions — a benchmark dataset will need to be merged separately\n",
    "    - Netflix's business model shifted dramatically over the dataset's time range (DVD-by-mail → streaming → global content), meaning the drivers of volatility differ significantly across eras\n",
    "    - Early IPO period (2002–2004) shows unusually high volatility due to thin trading volume and speculative small-cap behavior, which may skew comparisons against the mature-company period\n",
    "    - Adjusted closing prices in early years appear very small (~$1) due to retroactive split adjustments from Netflix's 2015 7-for-1 stock split\n",
    "- Dataset #5\n",
    "  - Dataset Name: Google.csv\n",
    "  - Link to the dataset: https://www.kaggle.com/datasets/aayushmishra1512/faang-complete-stock-data?resource=download&select=Google.csv\n",
    "  - Number of observations: 4041\n",
    "  - Number of variables: 7\n",
    "  - Most Relevant Variables: Date, High, Low, Open, Close, Adj Close, Volume\n",
    "  - Potential dataset limitations:\n",
    "    - The dataset only starts from Google's IPO in August 2004, so it doesn't capture any pre-IPO history.\n",
    "    - Price and volume data alone don't explain why prices move — earnings reports, product launches, and macroeconomic events are visible as patterns but are not labeled.\n",
    "    - The COVID-19 market disruption is only partially captured (early 2020 data is present but the full multi-year impact extends beyond the dataset).\n",
    "\n",
    "\n",
    "Each dataset deserves either a set of bullet points as above or a few sentences if you prefer that method.\n",
    "\n",
    "If you plan to use multiple datasets, add a few sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/airline-safety/airline-safety.csv', 'filename':'airline-safety.csv'},\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/bad-drivers/bad-drivers.csv', 'filename':'bad-drivers.csv'}\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netflix (NFLX) Daily Stock Prices\n",
    "\n",
    "This dataset contains daily historical stock market data for Netflix (NFLX) from May 2002 through mid-2020, spanning over 4,500 trading days. Each row represents one trading day — weekends and market holidays are excluded. All price columns are measured in **U.S. dollars (USD) per share**.\n",
    "\n",
    "- **Open** is the price at which Netflix stock first traded when the market opened that day.\n",
    "- **High** and **Low** are the highest and lowest prices reached during that trading session, giving a sense of intraday price range and turbulence.\n",
    "- **Close** is the final traded price at the end of the session.\n",
    "- **Adj Close** is the closing price adjusted for corporate actions, most importantly stock splits. Netflix underwent a 7-for-1 stock split in 2015, which is why early-era prices appear very small (around $1) — they have been retroactively scaled down for consistency. For our analysis, **Adj Close is the correct variable to use** when computing returns, as it produces a consistent and comparable price series across the full date range. From it we derive the **daily log return** (`log(Adj Close_t / Adj Close_{t-1})`), which measures how much the stock moved each day. Volatility is then quantified as the rolling standard deviation of those log returns — higher values signal greater day-to-day price swings and therefore higher risk.\n",
    "- **Volume** is the number of shares traded during the session (unit: shares). Unusually high volume days often coincide with earnings releases, major company news, or broad market stress events, and can serve as a secondary marker of market turbulence.\n",
    "\n",
    "There are a few notable concerns with this dataset. First, it contains only Netflix-specific price data and no external market variables such as S&P 500 returns or VIX levels. To classify market conditions as \"favorable\" or \"unfavorable\" for our analysis, we will need to merge in a separate benchmark dataset. Second, Netflix's business model changed dramatically across the dataset's time range — from a DVD-by-mail service (2002–2007), to a domestic streaming platform (2007–2016), to a global content producer (2016–present). The drivers of its stock volatility differ substantially across these eras, which is a meaningful confound when interpreting results across the full period. Third, the earliest records (2002–2004) reflect a newly public, thinly traded small-cap stock with speculative behavior, producing unusually high volatility that may not be representative of the mature company's risk profile. These early years should be interpreted with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# ── Paths ──────────────────────────────────────────────────────────────────────\n",
    "RAW_PATH      = Path(\"data/00-raw/Netflix.csv\")\n",
    "INTERIM_DIR   = Path(\"data/01-interim\")\n",
    "PROCESSED_DIR = Path(\"data/02-processed\")\n",
    "INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ── 1. Load ────────────────────────────────────────────────────────────────────\n",
    "df_raw = pd.read_csv(RAW_PATH)\n",
    "print(\"Raw shape:\", df_raw.shape)\n",
    "display(df_raw.head())\n",
    "\n",
    "# ── 2. Type enforcement ────────────────────────────────────────────────────────\n",
    "df = df_raw.copy()\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "num_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]\n",
    "for c in num_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nDataset size (rows, cols):\", df.shape)\n",
    "\n",
    "# ── 3. Missingness ─────────────────────────────────────────────────────────────\n",
    "missing_summary = pd.DataFrame({\n",
    "    \"missing_count\": df.isna().sum(),\n",
    "    \"missing_pct\":   (df.isna().sum() / len(df) * 100).round(4)\n",
    "}).sort_values(\"missing_count\", ascending=False)\n",
    "\n",
    "print(\"\\nMissingness summary:\")\n",
    "display(missing_summary)\n",
    "\n",
    "# Check whether missingness clusters in any particular year\n",
    "df[\"missing_any\"] = df.isna().any(axis=1)\n",
    "missing_by_year = df.groupby(df[\"Date\"].dt.year)[\"missing_any\"].mean()\n",
    "print(\"\\nMissing rate by year (only years with missing data shown):\")\n",
    "display(missing_by_year[missing_by_year > 0])\n",
    "\n",
    "# ── 4. Tidy check: OHLC internal consistency ──────────────────────────────────\n",
    "suspicious = df[\n",
    "    (df[\"Low\"]   > df[\"High\"]) |\n",
    "    (df[\"Open\"]  < df[\"Low\"])  | (df[\"Open\"]  > df[\"High\"]) |\n",
    "    (df[\"Close\"] < df[\"Low\"])  | (df[\"Close\"] > df[\"High\"])\n",
    "]\n",
    "print(f\"\\nSuspicious OHLC rows (internal inconsistencies): {len(suspicious)}\")\n",
    "if len(suspicious) > 0:\n",
    "    display(suspicious)\n",
    "\n",
    "# ── 5. Derived columns & outlier flagging ─────────────────────────────────────\n",
    "df[\"Daily_Log_Return\"] = np.log(df[\"Adj Close\"] / df[\"Adj Close\"].shift(1))\n",
    "df[\"Price_Range\"]      = df[\"High\"] - df[\"Low\"]\n",
    "\n",
    "# Extreme return days (beyond 1st/99th percentile)\n",
    "ret_lo, ret_hi = df[\"Daily_Log_Return\"].quantile([0.01, 0.99])\n",
    "extreme_ret = df[(df[\"Daily_Log_Return\"] <= ret_lo) | (df[\"Daily_Log_Return\"] >= ret_hi)]\n",
    "print(f\"\\nExtreme return thresholds — 1st pct: {ret_lo:.4f}, 99th pct: {ret_hi:.4f}\")\n",
    "print(f\"Extreme return days flagged: {len(extreme_ret)}\")\n",
    "display(extreme_ret[[\"Date\", \"Daily_Log_Return\", \"Adj Close\", \"Volume\"]].head(10))\n",
    "\n",
    "# Extreme volume days (above 99th percentile)\n",
    "vol_thresh = df[\"Volume\"].quantile(0.99)\n",
    "high_vol   = df[df[\"Volume\"] >= vol_thresh]\n",
    "print(f\"\\nHigh volume threshold (99th pct): {vol_thresh:,.0f} shares\")\n",
    "print(f\"High volume days flagged: {len(high_vol)}\")\n",
    "display(high_vol[[\"Date\", \"Volume\", \"Adj Close\", \"Daily_Log_Return\"]].head(10))\n",
    "\n",
    "# Note: extreme days in financial data represent real market events (earnings,\n",
    "# crashes, major announcements) — they are not errors and should NOT be removed,\n",
    "# as they are precisely the observations most relevant to our volatility analysis.\n",
    "\n",
    "# ── 6. Summary statistics ──────────────────────────────────────────────────────\n",
    "print(\"\\nSummary statistics — price & volume columns:\")\n",
    "display(df[num_cols].describe())\n",
    "\n",
    "print(\"\\nSummary statistics — daily log returns:\")\n",
    "display(df[[\"Daily_Log_Return\"]].describe())\n",
    "\n",
    "# ── 7. Visualizations ─────────────────────────────────────────────────────────\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "axes[0].plot(df[\"Date\"], df[\"Adj Close\"], color=\"royalblue\", linewidth=0.8)\n",
    "axes[0].set_title(\"Netflix Adjusted Closing Price Over Time\")\n",
    "axes[0].set_ylabel(\"Adj Close (USD)\")\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(df[\"Date\"], df[\"Daily_Log_Return\"], color=\"darkorange\", linewidth=0.5, alpha=0.8)\n",
    "axes[1].axhline(0, color=\"black\", linewidth=0.8, linestyle=\"--\")\n",
    "axes[1].set_title(\"Netflix Daily Log Returns\")\n",
    "axes[1].set_ylabel(\"Log Return\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "rolling_vol = df[\"Daily_Log_Return\"].rolling(window=30).std()\n",
    "axes[2].plot(df[\"Date\"], rolling_vol, color=\"firebrick\", linewidth=0.8)\n",
    "axes[2].set_title(\"Netflix 30-Day Rolling Volatility (Std of Log Returns)\")\n",
    "axes[2].set_ylabel(\"Volatility\")\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ── 8. Clean & save ───────────────────────────────────────────────────────────\n",
    "before = len(df)\n",
    "\n",
    "# Drop rows missing any core price column — these are unusable for return calculations.\n",
    "# We use dropna on the price columns only (not Volume, which has occasional gaps\n",
    "# that don't affect return computation). This is a conservative choice that drops\n",
    "# very few rows while ensuring no NaN values propagate into our return series.\n",
    "df_clean = df.dropna(subset=[\"Date\"] + num_cols).copy()\n",
    "df_clean = df_clean.drop(columns=[\"missing_any\"], errors=\"ignore\")\n",
    "\n",
    "after = len(df_clean)\n",
    "print(f\"\\nRows before cleaning: {before} | After: {after} | Dropped: {before - after}\")\n",
    "print(f\"Date range: {df_clean['Date'].min().date()} → {df_clean['Date'].max().date()}\")\n",
    "\n",
    "df.to_csv(INTERIM_DIR / \"netflix_interim.csv\", index=False)\n",
    "df_clean.to_csv(PROCESSED_DIR / \"netflix_processed.csv\", index=False)\n",
    "print(\"Files saved.\")\n",
    "\n",
    "display(df_clean.head())\n",
    "df_clean.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Daily Stock Prices\n",
    "\n",
    "This dataset contains daily historical stock market data for Google (Alphabet Inc.). Each row represents one trading day (so weekends and market holidays are excluded). The dataset includes standard market variables used in financial analysis: Open, High, Low, Close, Adjusted Close (Adj Close), and Volume, along with the Date.\n",
    "\n",
    "All price-related variables are measured in U.S. dollars (USD) per share.\n",
    "\n",
    "- Open is the first traded price when the market opens that day.\n",
    "- High and Low are the highest and lowest traded prices during the trading session.\n",
    "- Close is the last traded price at market close.\n",
    "- Adj Close is the closing price adjusted for corporate actions (primarily stock splits). For Google's data in this dataset (2004–2020), Close and Adj Close are identical throughout, since the 20-for-1 stock split Google performed occurred in 2022 till after this dataset ends. For return calculations over time, Adj Close is still the recommended column to use for consistency.\n",
    "- Volume is the number of shares traded that day (unit: shares). Higher volume often indicates increased investor attention, news/earnings events, or higher volatility. Google's daily volume tends to be much lower than other FAANG stocks in raw share count because its per-share price is significantly higher (reaching ~$1,700 by 2020).\n",
    "\n",
    "Major concerns/limitations: This dataset only includes market outcomes (prices/volume) and does not include explanatory variables like interest rates, inflation, sector indices, earnings surprises, or macroeconomic indicators. This means any \"why\" interpretation is limited unless additional datasets are incorporated. Additionally, the dataset begins at Google's IPO in August 2004, meaning it does not capture any pre-public company history, and the IPO period itself exhibits unusually high volatility typical of newly listed stocks. The dataset ends in September 2020, so it does not reflect more recent market conditions. Finally, \"outliers\" in finance are frequently real events (earnings, crashes, major announcements) rather than data errors, so outlier handling must be done carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Paths\n",
    "raw_path = \"data/00-raw/Google.csv\"\n",
    "interim_path = \"data/01-interim/google_interim.csv\"\n",
    "processed_path = \"data/02-processed/google_clean.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(raw_path, parse_dates=['Date'])\n",
    "print(\"Dataset loaded successfully.\\n\")\n",
    "\n",
    "# Check first few rows and columns\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "display(df.head())\n",
    "print(\"\\nColumns and data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check size of dataset\n",
    "print(f\"\\nDataset shape: {df.shape} (rows, columns)\")\n",
    "\n",
    "# Check missing values\n",
    "missing_counts = df.isnull().sum()\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# Identify outliers (z-score > 3)\n",
    "numeric_cols = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "for col in numeric_cols:\n",
    "    df[f'{col}_zscore'] = (df[col] - df[col].mean()) / df[col].std()\n",
    "\n",
    "# Flag extreme outliers\n",
    "outliers = df[(df[[f'{col}_zscore' for col in numeric_cols]]).abs() > 3].dropna(how='all')\n",
    "print(f\"\\nPotential outliers flagged: {len(outliers)}\")\n",
    "display(outliers)\n",
    "\n",
    "# Clean dataset: drop rows with missing Adj Close\n",
    "df_clean = df.dropna(subset=['Adj Close']).reset_index(drop=True)\n",
    "\n",
    "# Drop z-score columns\n",
    "df_clean = df_clean.drop(columns=[f'{col}_zscore' for col in numeric_cols])\n",
    "\n",
    "# Save intermediate and processed datasets\n",
    "df_clean.to_csv(interim_path, index=False)\n",
    "print(f\"Intermediate cleaned dataset saved to {interim_path}\")\n",
    "\n",
    "df_clean.to_csv(processed_path, index=False)\n",
    "print(f\"Processed dataset saved to {processed_path}\")\n",
    "\n",
    "# Summary after cleaning\n",
    "print(\"\\nSummary after cleaning:\")\n",
    "print(df_clean.info())\n",
    "print(df_clean.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Stock Dataset\n",
    "\n",
    "The dataset contains daily stock price information for Amazon from multiple years, including variables such as Date, Open, High, Low, Close, Adjusted Close, and Volume. Prices are recorded in U.S. dollars (USD) and represent the value of one share of Amazon stock at the opening, highest, lowest, or closing of each trading day. The Adjusted Close accounts for corporate actions like stock splits and dividends, providing a consistent measure for calculating daily returns. Volume indicates the number of shares traded each day, giving insight into market activity and liquidity. Key metrics for our analysis are the Adjusted Close and Volume. From the Adjusted Close, we calculate daily percentage returns and daily log returns, which are used to measure the stock’s volatility over time. Volatility is quantified as the standard deviation of daily log returns, with higher values indicating larger day-to-day price fluctuations. Trading volume, while not directly part of return calculations, can indicate periods of increased market interest or stress, which may correlate with higher volatility.\n",
    "\n",
    "Limitations: The dataset only includes Amazon stock and lacks broader market benchmarks like the S&P 500 or VIX, which are needed to define market conditions. It is limited to daily prices, omitting intraday fluctuations, and does not include external factors such as macroeconomic events or company news that could affect volatility. Additionally, because Amazon is a large tech company, its behavior may not generalize to smaller firms or other sectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Paths\n",
    "raw_path = \"data/00-raw/amazon.csv\"\n",
    "interim_path = \"data/01-interim/amazon_interim.csv\"\n",
    "processed_path = \"data/02-processed/amazon_clean.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(raw_path, parse_dates=['Date'])\n",
    "print(\"Dataset loaded successfully.\\n\")\n",
    "\n",
    "# Check first few rows and columns\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nColumns and data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check size of dataset\n",
    "print(f\"\\nDataset shape: {df.shape} (rows, columns)\")\n",
    "\n",
    "# Check missing values\n",
    "missing_counts = df.isnull().sum()\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# Identify outliers (z-score > 3)\n",
    "numeric_cols = ['Open', 'High', 'Low', 'Close', 'Adjusted Close', 'Volume']\n",
    "for col in numeric_cols:\n",
    "    df[f'{col}_zscore'] = (df[col] - df[col].mean()) / df[col].std()\n",
    "\n",
    "# Flag extreme outliers\n",
    "outliers = df[(df[[f'{col}_zscore' for col in numeric_cols]]).abs() > 3].dropna(how='all')\n",
    "print(f\"\\nPotential outliers flagged: {len(outliers)}\")\n",
    "display(outliers)\n",
    "\n",
    "# Clean dataset: drop rows with missing Adjusted Close\n",
    "df_clean = df.dropna(subset=['Adjusted Close']).reset_index(drop=True)\n",
    "\n",
    "# Drop z-score columns\n",
    "df_clean = df_clean.drop(columns=[f'{col}_zscore' for col in numeric_cols])\n",
    "\n",
    "# Save intermediate and processed datasets\n",
    "df_clean.to_csv(interim_path, index=False)\n",
    "print(f\"Intermediate cleaned dataset saved to {interim_path}\")\n",
    "\n",
    "df_clean.to_csv(processed_path, index=False)\n",
    "print(f\"Processed dataset saved to {processed_path}\")\n",
    "\n",
    "# Summary after cleaning\n",
    "print(\"\\nSummary after cleaning:\")\n",
    "print(df_clean.info())\n",
    "print(df_clean.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook (Meta) Daily Stock Prices\n",
    "\n",
    "This dataset contains daily historical stock market data for Facebook. Each row represents one trading day (so weekends and market holidays are excluded). The dataset includes standard market variables used in financial analysis: **Open, High, Low, Close, Adjusted Close (Adj Close), and Volume**, along with the **Date**.\n",
    "\n",
    "All price-related variables are measured in **U.S. dollars (USD)** per share.  \n",
    "- **Open** is the first traded price when the market opens that day.  \n",
    "- **High** and **Low** are the highest and lowest traded prices during the trading session.  \n",
    "- **Close** is the last traded price at market close.  \n",
    "- **Adj Close** is the closing price adjusted for corporate actions (primarily **stock splits**; dividends are generally not relevant for FB historically). For return calculations over time, **Adj Close** is typically the most appropriate measure.  \n",
    "- **Volume** is the number of shares traded that day (unit: **shares**). Higher volume often indicates increased investor attention, news/earnings events, or higher volatility.\n",
    "\n",
    "Major concerns / limitations: This dataset only includes **market outcomes** (prices/volume) and does not include explanatory variables like interest rates, inflation, sector indices, earnings surprises, or macroeconomic indicators. This means any “why” interpretation is limited unless we add additional datasets later. Also, stock data often shows **non-stationarity** (long-term trend changes over time), and early trading periods (e.g., IPO era) can have unusually high volatility. Finally, “outliers” in finance are frequently real events (earnings, crashes, major announcements) rather than data errors, so outlier handling must be done carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = input): <text>:1:8: unexpected symbol\n1: import pandas\n           ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = input): <text>:1:8: unexpected symbol\n1: import pandas\n           ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_PATH = Path(\"data/00-raw/Facebook.csv\")\n",
    "INTERIM_DIR = Path(\"data/01-interim\")\n",
    "PROCESSED_DIR = Path(\"data/02-processed\")\n",
    "\n",
    "INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_raw = pd.read_csv(RAW_PATH)\n",
    "\n",
    "print(\"Raw shape:\", df_raw.shape)\n",
    "display(df_raw.head())\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "num_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]\n",
    "for c in num_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "print(\"\\nAfter type enforcement:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nDataset size (rows, cols):\", df.shape)\n",
    "\n",
    "missing_counts = df.isna().sum()\n",
    "missing_pct = (missing_counts / len(df)) * 100\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    \"missing_count\": missing_counts,\n",
    "    \"missing_pct\": missing_pct\n",
    "}).sort_values(\"missing_count\", ascending=False)\n",
    "\n",
    "print(\"\\nMissingness summary:\")\n",
    "display(missing_summary)\n",
    "\n",
    "df[\"missing_any\"] = df.isna().any(axis=1)\n",
    "missing_by_year = df.groupby(df[\"Date\"].dt.year)[\"missing_any\"].mean()\n",
    "\n",
    "print(\"\\nFraction of rows with any missing value by year:\")\n",
    "display(missing_by_year)\n",
    "\n",
    "df[\"Daily_Return\"] = df[\"Adj Close\"].pct_change()\n",
    "df[\"Price_Range\"] = df[\"High\"] - df[\"Low\"]\n",
    "\n",
    "suspicious_price_rows = df[\n",
    "    (df[\"Low\"] > df[\"High\"]) |\n",
    "    (df[\"Open\"] < df[\"Low\"]) | (df[\"Open\"] > df[\"High\"]) |\n",
    "    (df[\"Close\"] < df[\"Low\"]) | (df[\"Close\"] > df[\"High\"])\n",
    "]\n",
    "\n",
    "print(\"\\nSuspicious price rows (should be near zero):\", len(suspicious_price_rows))\n",
    "if len(suspicious_price_rows) > 0:\n",
    "    display(suspicious_price_rows.head())\n",
    "\n",
    "vol_threshold = df[\"Volume\"].quantile(0.99)\n",
    "high_volume_days = df[df[\"Volume\"] >= vol_threshold]\n",
    "\n",
    "print(\"\\nHigh volume threshold (99th percentile):\", vol_threshold)\n",
    "print(\"Number of high-volume days flagged:\", len(high_volume_days))\n",
    "display(high_volume_days[[\"Date\", \"Volume\", \"Open\", \"Close\"]].head())\n",
    "\n",
    "ret_low = df[\"Daily_Return\"].quantile(0.01)\n",
    "ret_high = df[\"Daily_Return\"].quantile(0.99)\n",
    "extreme_return_days = df[(df[\"Daily_Return\"] <= ret_low) | (df[\"Daily_Return\"] >= ret_high)]\n",
    "\n",
    "print(\"\\nExtreme return thresholds (1st, 99th pct):\", (ret_low, ret_high))\n",
    "print(\"Number of extreme-return days flagged:\", len(extreme_return_days))\n",
    "display(extreme_return_days[[\"Date\", \"Daily_Return\", \"Adj Close\", \"Volume\"]].head())\n",
    "\n",
    "before = len(df)\n",
    "\n",
    "df_clean = df.dropna(subset=[\"Date\"] + num_cols).copy()\n",
    "\n",
    "after = len(df_clean)\n",
    "print(f\"\\nRows before cleaning: {before}, after cleaning: {after}, dropped: {before - after}\")\n",
    "\n",
    "df_clean = df_clean.drop(columns=[\"missing_any\"], errors=\"ignore\")\n",
    "\n",
    "interim_path = INTERIM_DIR / \"facebook_interim_typed.csv\"\n",
    "df.to_csv(interim_path, index=False)\n",
    "\n",
    "processed_path = PROCESSED_DIR / \"facebook_processed.csv\"\n",
    "df_clean.to_csv(processed_path, index=False)\n",
    "\n",
    "print(\"\\nWrote interim to:\", interim_path)\n",
    "print(\"Wrote processed to:\", processed_path)\n",
    "\n",
    "display(df_clean.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data Collection\n",
    " - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
    "\n",
    " This item is not applicable to our project. Our data consists entirely of publicly traded stock market prices and volumes recorded on regulated exchanges. There are no human subjects, surveys, or personally identifiable information involved in data collection.\n",
    "\n",
    " - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
    "\n",
    " Although stock data itself is objective, limiting the analysis to FAANG companies introduces selection bias because these firms are unusually large, well-capitalized, and influential. They were chosen in part because they are well-known success stories, which means our sample systematically excludes companies that failed, were acquired, or never achieved this scale — a form of survivorship bias. Our findings may not generalize to smaller technology companies, other sectors, or companies that did not survive comparable market downturns.\n",
    "\n",
    " - [X] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    "\n",
    " This item is not applicable. Our dataset contains only publicly available financial market data (prices, volume, dates) with no personally identifiable information of any kind.\n",
    "\n",
    " - [X] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    "\n",
    " This item is not directly applicable since our data involves companies rather than individuals. However, we acknowledge that stock market participation and wealth derived from equity markets is not evenly distributed across demographic groups, so conclusions drawn from FAANG performance may have unequal relevance to different communities.\n",
    "\n",
    "### B. Data Storage\n",
    " - [X] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    "\n",
    " Since all data used in this project is publicly available from Kaggle and financial markets, the security risk is minimal. Data is stored in a shared GitHub repository accessible only to team members, and no sensitive or private data is included.\n",
    "\n",
    " - [X] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    "\n",
    " Not applicable. Our dataset contains no personal information about any individual, so no removal mechanism is necessary.\n",
    "\n",
    " - [X] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "\n",
    " The raw data files will be retained only for the duration of the project. Once the final report is submitted and graded, local copies of the data will be deleted. Since the data is publicly available on Kaggle, there is no concern about proprietary retention.\n",
    "\n",
    "### C. Analysis\n",
    " - [X] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
    "\n",
    " Focusing only on stock performance may overlook broader economic factors such as government monetary policy, geopolitical events, regulatory changes, or industry-level disruptions that independently drive volatility. For example, Netflix's volatility may be influenced by streaming competition trends that are unrelated to general market conditions. We will acknowledge these external influences when interpreting results and avoid overstating causal claims.\n",
    "\n",
    " - [X] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
    "\n",
    " Market conditions and time period selection could significantly influence conclusions. To reduce this risk, we will use at least 10 years of data to capture multiple economic cycles including bull markets, bear markets, and crisis periods such as the 2008 financial crisis and the COVID-19 pandemic. We also acknowledge that our classification of \"favorable\" vs. \"unfavorable\" market conditions using S&P 500 returns or VIX is itself a modeling choice that could affect results, and we will test sensitivity to different classification thresholds.\n",
    "\n",
    " - [X] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
    "\n",
    " Visualizations and summary statistics will be designed to accurately reflect the data without exaggerating trends or selectively presenting results. Axes will be clearly labeled with units, time ranges will be stated explicitly, and we will avoid cherry-picking time windows that make results appear more dramatic than they are. Where uncertainty exists, we will represent it clearly using confidence intervals or explicit caveats.\n",
    "\n",
    " - [X] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
    "\n",
    " Our dataset contains no PII. All data displayed in the analysis — prices, volumes, and dates — is publicly available financial market information. No individual-level data is present at any stage of the analysis.\n",
    "\n",
    " - [X] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
    "\n",
    " All data cleaning, transformation, and analysis steps are documented in reproducible Python code within the project notebook. Raw data is preserved separately from processed data, and intermediate steps are saved so that any stage of the pipeline can be re-examined or corrected without rerunning the full workflow.\n",
    "\n",
    "### D. Modeling\n",
    " - [X] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
    "\n",
    " Our analysis involves only stock market data for publicly traded companies and does not involve any demographic or individual-level variables. There are no protected group proxies in our dataset, and discrimination is not a relevant concern for this type of financial time series analysis.\n",
    "\n",
    " - [X] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
    "\n",
    " This item is not directly applicable since our units of analysis are companies, not people. However, we acknowledge that any findings about FAANG stock stability could be interpreted differently by different types of investors (retail vs. institutional), and we will frame our results carefully to avoid implying that these stocks are appropriate investments for any particular group.\n",
    "\n",
    " - [X] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
    "\n",
    " We will consider multiple metrics such as daily log returns, rolling standard deviation of returns, and trading volume rather than relying on a single measure, ensuring a more balanced evaluation of company performance. We will also consider whether results hold under different volatility window sizes (e.g., 20-day vs. 60-day rolling windows) to avoid sensitivity to an arbitrary methodological choice.\n",
    "\n",
    " - [X] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    "\n",
    " The statistical methods used — including standard deviation of log returns, regime classification via S&P 500 thresholds, and comparative hypothesis tests — are interpretable and well-established in financial analysis. We will explain each step in plain language so that conclusions are understandable to a general audience, not just those with a finance background.\n",
    "\n",
    " - [X] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
    "\n",
    " We will explicitly state that past stock performance does not guarantee future results and that our analysis should not be interpreted as financial advice. We will also note the limitations of using only five companies, the survivorship bias in our sample, and the fact that our market regime classifications are based on simplified thresholds rather than formal economic definitions.\n",
    "\n",
    "### E. Deployment\n",
    " - [X] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    "\n",
    " This project is a one-time academic analysis rather than a deployed production system, so ongoing monitoring is not applicable. However, if the analysis were extended or updated with new data, results should be re-evaluated since stock market dynamics and company business models evolve over time, and conclusions drawn from historical data may not hold in new market regimes.\n",
    "\n",
    " - [X] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    "\n",
    " Since this is an academic project, formal redress mechanisms are not in place. However, we will clearly label all outputs as exploratory and educational, and explicitly disclaim any investment advice interpretation, reducing the likelihood that readers act on the findings in ways that could cause financial harm.\n",
    "\n",
    " - [X] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    "\n",
    " Not applicable for a static academic analysis. All analysis steps are version-controlled via GitHub, so any prior state of the project can be restored if needed.\n",
    "\n",
    " - [X] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n",
    "\n",
    " There is a possibility that readers could misuse the findings for investment decisions, particularly if results suggest one FAANG company is consistently \"safer\" than others during downturns. We will clarify prominently that the project is exploratory and educational, that it covers a specific historical period, and that it is not a recommendation to buy or sell any security. We will also note that past volatility patterns do not reliably predict future behavior."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Don’t wait till the last minute to do the work (12 hours before)\n",
    "* *Respond in a timely manner 2*\n",
    "* *Come to the once-a-week meetings\n",
    "* *Make sure to communicate proactivly about ideas and feedback in a respectable manner\n",
    "* *if disagreement, actively work on finding a compromise with the whole group. If no compromise is reached, then we go with the majority\n",
    "* *If someone is struggling with a section or a deadline, make sure to communicate issues with the group  through i messages 24 hour before so we can support them*\n",
    "* *Be aware of the assignment deadlines*\n",
    "* *We will have a spreadsheet and will update it on a spreadsheet when a task is worked on, and meet in person to communicate our progress\n",
    "* *We will split the work equally by effort for said task*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/4  |  4 PM | Look over the project proposal and pick potential research topics  | Finalize the research question and complete the proposal. Also discuss communication and team expecatations. | \n",
    "| 2/13  |  4 PM |  Conduct background research on volatility and correlation in financial markets | Discuss the background and other potential confounding varibales, refine the hypothesis and outline ideal dataset and ethics considerations  | \n",
    "| 2/20  | 4 PM  | Locate potential datasets (FAANG stock data, market benchmarks) | locate potential datasets (FAANG stock data, market benchmarks) |\n",
    "| 2/27  | 4 PM  | Immport and clean FAANG stock data and compute returns | Review data cleaning and begin  data analysis (price trends, returns, basic volatility plots) |\n",
    "| 3/6  | 4 PM  | Complete data analysis and compute rolling volatility and rolling correlations | Discuss/edit Analysis and Complete project check-in |\n",
    "| 3/13  | 4 PM  | Conduct full analysis (volatility comparison, correlation dynamics, regime comparison)| Review and interpret analysis results; discuss limitations and robustness |\n",
    "| 3/18  | Before 11:59 PM  | Draft results, discussion, and conclusion sections | Edit and refine full project and align findings with research question and hypothesis |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
